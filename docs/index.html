<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Building an Open-Source Verilog Simulator with AI: 580K Lines in 43 Days</title>
<style>
  :root {
    --bg: #ffffff;
    --text: #1a1a2e;
    --text-secondary: #4a4a6a;
    --accent: #2563eb;
    --accent-light: #eff6ff;
    --border: #e5e7eb;
    --code-bg: #f8f9fc;
    --code-border: #e2e4e9;
    --heading: #0f172a;
    --max-width: 720px;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans', Helvetica, Arial, sans-serif;
    color: var(--text);
    background: var(--bg);
    line-height: 1.7;
    font-size: 17px;
    -webkit-font-smoothing: antialiased;
  }

  .hero {
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #0f172a 100%);
    color: #fff;
    padding: 80px 24px 64px;
    text-align: center;
  }
  .hero h1 {
    font-size: clamp(28px, 4.5vw, 44px);
    font-weight: 800;
    line-height: 1.15;
    max-width: 800px;
    margin: 0 auto 20px;
    letter-spacing: -0.02em;
  }
  .hero .subtitle {
    font-size: 18px;
    color: #94a3b8;
    max-width: 600px;
    margin: 0 auto;
    line-height: 1.6;
  }
  .hero .meta {
    margin-top: 24px;
    font-size: 14px;
    color: #64748b;
  }
  .hero .meta a { color: #60a5fa; text-decoration: none; }
  .hero .meta a:hover { text-decoration: underline; }

  .stats-bar {
    background: #f8fafc;
    border-bottom: 1px solid var(--border);
    padding: 20px 24px;
  }
  .stats-bar .inner {
    max-width: 900px;
    margin: 0 auto;
    display: flex;
    justify-content: center;
    gap: 48px;
    flex-wrap: wrap;
  }
  .stat {
    text-align: center;
  }
  .stat .number {
    font-size: 28px;
    font-weight: 800;
    color: var(--accent);
    letter-spacing: -0.02em;
  }
  .stat .label {
    font-size: 13px;
    color: var(--text-secondary);
    text-transform: uppercase;
    letter-spacing: 0.05em;
    margin-top: 2px;
  }

  article {
    max-width: var(--max-width);
    margin: 0 auto;
    padding: 48px 24px 80px;
  }

  h2 {
    font-size: 28px;
    font-weight: 700;
    color: var(--heading);
    margin: 56px 0 20px;
    letter-spacing: -0.01em;
    line-height: 1.25;
  }
  h2:first-of-type { margin-top: 0; }

  h3 {
    font-size: 20px;
    font-weight: 600;
    color: var(--heading);
    margin: 36px 0 12px;
  }

  p {
    margin-bottom: 16px;
    color: var(--text);
  }

  a {
    color: var(--accent);
    text-decoration: none;
  }
  a:hover { text-decoration: underline; }

  strong { font-weight: 600; }
  em { font-style: italic; }

  hr {
    border: none;
    border-top: 1px solid var(--border);
    margin: 48px 0;
  }

  ul, ol {
    margin: 0 0 16px 24px;
  }
  li {
    margin-bottom: 6px;
  }
  li strong {
    color: var(--heading);
  }

  .timeline-block {
    margin: 20px 0;
  }
  .timeline-block p {
    font-weight: 600;
    color: var(--heading);
    margin-bottom: 6px;
  }
  .timeline-block ul {
    margin-top: 4px;
  }

  pre {
    background: var(--code-bg);
    border: 1px solid var(--code-border);
    border-radius: 8px;
    padding: 16px 20px;
    overflow-x: auto;
    margin: 16px 0 20px;
    font-size: 14px;
    line-height: 1.6;
  }
  code {
    font-family: 'SF Mono', 'Fira Code', 'Fira Mono', Menlo, Consolas, monospace;
    font-size: 0.9em;
  }
  p code, li code {
    background: var(--code-bg);
    border: 1px solid var(--code-border);
    border-radius: 4px;
    padding: 1px 5px;
    font-size: 0.85em;
  }

  figure {
    margin: 28px 0;
  }
  figure img {
    width: 100%;
    max-width: 100%;
    border-radius: 8px;
    border: 1px solid var(--border);
    display: block;
  }
  figure figcaption {
    text-align: center;
    font-size: 14px;
    color: var(--text-secondary);
    margin-top: 8px;
  }

  /* Wide figures that break out of the text column */
  figure.wide {
    max-width: 900px;
    margin-left: calc((var(--max-width) - 900px) / 2);
    margin-right: calc((var(--max-width) - 900px) / 2);
  }
  @media (max-width: 948px) {
    figure.wide {
      max-width: 100%;
      margin-left: -24px;
      margin-right: -24px;
    }
    figure.wide img {
      border-radius: 0;
      border-left: none;
      border-right: none;
    }
  }

  table {
    width: 100%;
    border-collapse: collapse;
    margin: 16px 0 24px;
    font-size: 15px;
  }
  thead th {
    text-align: left;
    font-weight: 600;
    color: var(--heading);
    padding: 10px 12px;
    border-bottom: 2px solid var(--border);
    font-size: 14px;
    text-transform: uppercase;
    letter-spacing: 0.03em;
  }
  tbody td {
    padding: 10px 12px;
    border-bottom: 1px solid var(--border);
  }
  tbody tr:last-child td {
    border-bottom: none;
  }
  tbody tr:hover {
    background: var(--accent-light);
  }

  .callout {
    background: var(--accent-light);
    border-left: 4px solid var(--accent);
    border-radius: 0 8px 8px 0;
    padding: 16px 20px;
    margin: 20px 0;
  }
  .callout p { margin: 0; }

  .footer-note {
    margin-top: 48px;
    padding-top: 24px;
    border-top: 1px solid var(--border);
    font-size: 15px;
    color: var(--text-secondary);
    font-style: italic;
  }

  @media (max-width: 600px) {
    body { font-size: 16px; }
    .hero { padding: 48px 16px 40px; }
    .hero h1 { font-size: 26px; }
    article { padding: 32px 16px 48px; }
    h2 { font-size: 24px; margin-top: 40px; }
    .stats-bar .inner { gap: 24px; }
    .stat .number { font-size: 22px; }
  }
</style>
</head>
<body>

<div class="hero">
  <h1>Building an Open-Source Verilog Simulator with AI: 580K Lines in 43 Days</h1>
  <p class="subtitle">
    How one engineer used Claude and Codex to build a practical verification stack on top of CIRCT &mdash;
    simulation, formal verification, mutation testing, and more.
  </p>
  <p class="meta">
    Thomas Ahle &middot; February 2026 &middot;
    <a href="https://github.com/thomasnormal/circt">github.com/thomasnormal/circt</a>
  </p>
</div>

<div class="stats-bar">
  <div class="inner">
    <div class="stat"><div class="number">2,968</div><div class="label">Commits</div></div>
    <div class="stat"><div class="number">580K</div><div class="label">Lines Added</div></div>
    <div class="stat"><div class="number">43</div><div class="label">Days</div></div>
    <div class="stat"><div class="number">4,229</div><div class="label">Test Files</div></div>
    <div class="stat"><div class="number">40</div><div class="label">Regression Lanes</div></div>
  </div>
</div>

<article>

<p>
  Commercial Verilog simulators like Cadence Xcelium and Synopsys VCS cost hundreds of thousands of dollars per seat per year.
  For most hardware teams, these tools are the single largest line item in their EDA budget &mdash; and yet the core algorithms are well-understood, the specifications are public (IEEE 1800-2017), and open-source compiler infrastructure like <a href="https://circt.llvm.org/">CIRCT</a> already exists.
</p>

<p>What if you could close the gap with agentic AI?</p>

<p>
  Over 43 days in January and February 2026, I used Claude and Codex to land <strong>2,968 commits</strong> on a CIRCT fork &mdash;
  adding a full event-driven simulator, VPI/cocotb integration, UVM runtime support, bounded model checking, logic equivalence checking, and mutation testing.
  The result is a practical, open-source verification stack that can simulate real-world protocol testbenches end-to-end.
</p>

<p>This post is a technical account of what happened, what the numbers look like, and what I think it means.</p>

<hr>

<h2>What is CIRCT, and What Was Missing?</h2>

<p>
  <a href="https://circt.llvm.org/">CIRCT</a> (Circuit IR Compilers and Tools) is an LLVM/MLIR-based infrastructure for hardware design and verification.
  It provides a rich set of intermediate representations &mdash; HW, Comb, Seq, LLHD, Moore &mdash; and tools for parsing Verilog (<code>circt-verilog</code>, powered by <a href="https://github.com/MikePopoloski/slang">slang</a>), optimizing IR (<code>circt-opt</code>), and basic formal checking (<code>circt-bmc</code>, <code>circt-lec</code>).
</p>

<p>
  What it did <em>not</em> have was a practical simulation runtime.
  You could parse Verilog into MLIR and lower it through various dialects, but you couldn't actually <em>run</em> a testbench.
  There was no event-driven scheduler, no VPI layer, no UVM support, no waveform output, no cocotb integration.
  The gap between &ldquo;we can compile Verilog&rdquo; and &ldquo;we can simulate a design&rdquo; was enormous.
</p>

<p>
  Bridging that gap is exactly the kind of task where agentic AI shines: the specification is known (IEEE 1800), the interfaces are well-defined, and the work is largely <em>volume</em> &mdash; thousands of op handlers, system calls, edge cases &mdash; rather than fundamental research.
</p>

<hr>

<h2>By the Numbers</h2>

<p>
  The fork adds <strong>580,430 lines</strong> across 3,846 files, with only 10,985 lines removed from upstream.
  The chart below shows the full story: cumulative lines of code (blue), test file count (purple), and weekly commit velocity (orange numbers along the bottom), with color-coded development phases and milestone annotations.
</p>

<figure class="wide">
  <img src="chart_timeline.png" alt="Dense development timeline showing lines of code, test files, commits per week, and feature milestones over 43 days">
  <figcaption>Development timeline: LOC growth, test file count, weekly commit velocity, and feature milestones. Phase bands show the progression from foundation through hardening.</figcaption>
</figure>

<p>
  The pace started at ~25 commits/day in week 1 and peaked at <strong>124 commits/day</strong> in week 7 (Feb 10&ndash;16).
  This isn't because the AI got faster &mdash; it's because the later work was more mechanical (regression infrastructure, test harnesses, quality gates) while the earlier work required more design iteration.
  Test files grew from 987 (upstream baseline) to <strong>4,229</strong> &mdash; a 4.3x increase, with a sharp inflection around Feb 6 when formal and mutation suites came online.
</p>

<h3>Where the Work Went</h3>

<p>Every one of the 2,968 commits is accounted for in a named category:</p>

<figure class="wide">
  <img src="chart_work_areas.png" alt="All 2,968 commits broken down into 14 categories, with formal verification (652) as the largest">
  <figcaption>Full commit breakdown. Formal verification leads at 652 commits, followed by docs/iteration logs (521), Verilog frontend (461), mutation testing (372), and simulation engine (367).</figcaption>
</figure>

<p>
  Formal verification (BMC + LEC) and mutation testing together account for over 1,000 commits &mdash; 34% of the total.
  The &ldquo;Docs &amp; iteration logs&rdquo; category (521) reflects the project&rsquo;s 1,554-iteration engineering log, which tracked every AI interaction cycle.
  The Verilog frontend (461 commits) covers <code>ImportVerilog</code> and <code>MooreToCore</code>, the two major lowering passes that convert parsed Verilog into simulatable IR.
</p>

<hr>

<h2>How We Worked with AI</h2>

<figure>
  <img src="chart_ai_attribution.png" alt="AI attribution: 40% Claude Opus 4.5, 14% Claude Opus 4.6, 46% Codex" style="max-width: 420px; margin: 0 auto;">
</figure>

<p>
  <strong>Claude</strong> handled 54% of commits, in the beginning using Opus 4.5 and a custom StopHook to ensure it would keep going.
  When Opus 4.6 came out, I switched to that, and with team mode I no longer needed to force it to continue, it would just go on forever.
</p>

<p>
   At first Codex wasn't much help.
   Codex 5.2 didn't like working with other agents and would keep reverting their changes.
   However 5.3 changed that, and it quickly became a valueable team member.
   While it lacked the team coordination of Claude, it made up for it with the ability to deeply think about problems in xhigh mode.
   I used a <a href="https://github.com/thomasnormal/codex-auto-continue">a custom utility</a> to keep it going.
</p>

<p>
   Since I was on paternity leave during this work, I didn't have a lot of time to check in on progress.
   But when I did, it was mostly to check if the agents had crashed my server.
   Compiling and running verilog can quickly use up more than the 64GB memory I had, or 256GB hard drive space.
   Later I would mostly instruct on higher level tasks, like new datasets to test on, and features like JIT to improve simulation bottlenecks.
</p>

<p>
  I tried to have the agents share an engineering log file with their progress or issues, which they mostly were able to.
  The project log records <strong>1,554 iterations</strong> of this cycle.
  Many features took 3&ndash;5 iterations from first attempt to passing tests.
  Some &mdash; particularly UVM sequencer integration and VPI callback handling &mdash; took 20+.
</p>

<h3>What AI Was Good At</h3>

<ul>
  <li><strong>Op handlers</strong>: CIRCT has dozens of MLIR operations that need lowering to simulation behavior.
    Writing the 50th <code>hw::ArrayGetOp</code> handler is tedious but well-specified.
    AI handles this effortlessly.</li>
  <li><strong>Test generation</strong>: Given a feature spec and existing test patterns, AI generates thorough test cases faster than I could enumerate them.
    The 3,200+ new test files are almost entirely AI-generated.</li>
  <li><strong>Regression infrastructure</strong>: Shell scripts for running test suites, parsing results, tracking baselines, retrying flaky tests.
    This is exactly the kind of boilerplate that AI produces reliably.</li>
  <li><strong>Debugging from error output</strong>: When a test fails, the AI can read the MLIR dump, compare expected vs actual, and often identify the root cause faster than manual inspection.</li>
</ul>

<h3>What AI Was Bad At</h3>

<ul>
  <li><strong>Architectural decisions</strong>: The AI would happily implement whatever I asked for, even if the design was wrong.
    Choosing <em>where</em> to put the VPI layer, how to structure the event queue, when to use interpret vs JIT &mdash; these required human judgment.</li>
  <li><strong>Cross-cutting invariants</strong>: When a change in the process scheduler affected both the VPI callback order and the UVM phase sequencer, the AI would fix one and break the other.
    Maintaining consistency across subsystems required constant human oversight.</li>
  <li><strong>Performance</strong>: The AI optimizes for correctness, not speed.
    Several early implementations had O(n&sup2;) behavior that only surfaced on larger designs.</li>
</ul>

<hr>

<h2>What Works Today</h2>

<p>Here's what you can actually <em>do</em> with this fork that you can't do with upstream CIRCT:</p>

<h3>Simulate a SystemVerilog Design</h3>

<pre><code><span style="color:#6b7280"># Compile Verilog to MLIR</span>
circt-verilog mydesign.sv -o mydesign.mlir

<span style="color:#6b7280"># Run event-driven simulation</span>
circt-sim mydesign.mlir --top top_module --max-time 10000000</code></pre>

<p>
  Upstream CIRCT can parse the Verilog, but has no production simulator.
  This fork provides a full IEEE 1800-2017 event-driven simulator with four-state logic, VCD waveform output, and DPI-C support.
</p>

<h3>Run cocotb Testbenches</h3>

<pre><code><span style="color:#6b7280"># Run with cocotb via VPI</span>
COCOTB_TEST_MODULES="my_test" \
COCOTB_TOPLEVEL="dut" \
circt-sim mydesign.mlir --top dut \
  --vpi "$COCOTB_ROOT/libs/libcocotbvpi_ius.so"</code></pre>

<p>
  The VPI layer implements enough of the IEEE 1364/1800 C API for cocotb to discover signals, schedule callbacks, read/write values, and drive test sequences.
  The cocotb regression suite covers 50+ test scenarios.
</p>

<h3>Run UVM Testbenches (AVIP Protocol Suites)</h3>

<p>
  The fork can run real UVM testbenches &mdash; not toy examples, but full protocol verification IP.
  We test against <a href="https://github.com/mbits-mirafra">Mirafra&rsquo;s open-source AVIPs</a> (Advanced Verification IP) &mdash; UVM testbenches for standard bus and communication protocols including APB, AHB, AXI4, SPI, JTAG, I2S, and I3C.
  Each AVIP contains a complete UVM environment with drivers, monitors, scoreboards, coverage collectors, and constrained-random sequences.
  To put the results in context, here&rsquo;s how circt-sim compares to Cadence Xcelium on the same AVIP testbenches:
</p>

<figure class="wide">
  <img src="chart_avip_comparison.png" alt="AVIP protocol coverage comparison between circt-sim and Xcelium across 7 protocols">
  <figcaption>Coverage comparison across 7 AVIP protocols. circt-sim exceeds Xcelium on APB and SPI, matches on I3C, and falls short on AHB. AXI4 and JTAG fail to compile.</figcaption>
</figure>

<p>
  APB is the standout: circt-sim achieves <strong>55% coverage vs Xcelium&rsquo;s 25%</strong> &mdash; the open-source simulator actually exercises more of the protocol space.
  SPI shows a similar pattern (38% vs 19%).
  AHB is the biggest gap: Xcelium hits 86% while circt-sim stalls at 50% due to a monitor that never sees transactions advance past IDLE state.
  AXI4 and JTAG fail at compile time (the frontend produces empty MLIR files for these particular testbenches).
</p>

<h3>The Speed Gap (and How We&rsquo;re Closing It)</h3>

<p>
  Let&rsquo;s be honest about performance.
  In interpret mode &mdash; where circt-sim walks the MLIR IR operation by operation &mdash; the speed difference with commercial tools is dramatic:
</p>

<figure class="wide">
  <img src="chart_speed_comparison.png" alt="Performance comparison showing circt-sim interpret and compile modes vs Xcelium">
  <figcaption>Left: compile time. Right: simulation wall time on log scale. Green bars show JIT compile mode where it runs successfully; &ldquo;crash&rdquo; marks protocols where the JIT still has incomplete coverage.</figcaption>
</figure>

<p>
  In interpret mode, simulation is <strong>1,500x to 60,000x slower</strong> than Xcelium depending on the protocol.
  That&rsquo;s not a typo.
  The root cause is that every simulation cycle, the interpreter fetches an MLIR operation, runs ~20 type checks to identify it, looks up each operand in a hash map, computes the result using heap-allocated arbitrary-precision integers (even for a 1-bit wire), and stores it back.
  Xcelium compiles to native machine code where a 32-bit add is one instruction.
  The per-operation overhead is 50&ndash;100x, and it compounds.
</p>

<p>
  This gap is not acceptable &mdash; a 1,000x-slower simulator is a toy, not a tool.
  The path to parity is <strong>compile mode</strong>.
</p>

<h3>Closing the Gap: The JIT Problem</h3>

<p>
  CIRCT already has a fast simulation backend: <strong>arcilator</strong>.
  It compiles an entire design into a single native function via LLVM&rsquo;s ORC JIT engine, and calls it once per clock cycle.
  This works well for combinational and simple sequential logic &mdash; it&rsquo;s how you&rsquo;d simulate a counter or an ALU at near-native speed.
</p>

<p>
  But arcilator is a <strong>cycle-based</strong> simulator.
  It has no event-driven scheduler, no concurrent processes, no <code>fork</code>/<code>join</code>, no <code>#delay</code>.
  That means no UVM, no protocol testbenches, no behavioral SystemVerilog &mdash; exactly the things circt-sim was built to handle.
  The hard problem isn&rsquo;t compiling arithmetic to native code (arcilator already does that).
  It&rsquo;s compiling <em>processes that suspend and resume</em>.
</p>

<p>
  An event-driven process runs until it hits <code>@(posedge clk)</code> or <code>#10ns</code>, then suspends &mdash; saving its local variables, program counter, and stack frame &mdash; and yields to the scheduler.
  When the scheduler determines the process&rsquo;s trigger has fired, it resumes execution from where it left off.
  This suspend/resume pattern is fundamental to Verilog semantics, and it&rsquo;s what makes JIT compilation genuinely hard.
</p>

<p>There are three viable approaches:</p>

<ol>
  <li><strong>Coroutine-based</strong> &mdash; compile each process to an LLVM coroutine.
    <code>llhd.wait</code> becomes a coroutine suspend; the scheduler resumes it.
    LLVM handles saving/restoring state automatically.
    Cleanest abstraction, but LLVM&rsquo;s coroutine machinery was designed for C++20 <code>co_await</code>, not hardware processes, and may fight the semantics.</li>
  <li><strong>State-machine transformation</strong> &mdash; rewrite each process as a state machine at the IR level.
    Every wait point becomes a state boundary; the compiled function takes a state pointer, runs until the next wait, and returns.
    No coroutine dependency, but the IR transformation is complex &mdash; SSA values live across wait points must be explicitly spilled to a state struct.</li>
  <li><strong>Block-level JIT</strong> &mdash; don&rsquo;t compile whole processes.
    Instead, identify hot basic blocks (clock toggles, BFM driver loops, scoreboard arithmetic) and compile just those to native code.
    The interpreter still handles control flow and suspension, but inner loops run at native speed.
    The most incremental approach &mdash; it builds on the existing infrastructure and targets the code that dominates execution time.</li>
</ol>

<p>
  The fork currently includes a <code>--mode=compile</code> flag with a deoptimization framework that allows controlled fallback from compiled code to the interpreter.
  Today this produces a <strong>2.1x speedup</strong> on I2S, with most processes still falling back.
  The real gains will come from implementing one of the three approaches above to eliminate interpreter dispatch from hot paths entirely.
</p>

<p>
  The 1,000x gap to Xcelium breaks down roughly as: <strong>50&ndash;100x</strong> from per-operation interpreter dispatch (what JIT fixes), <strong>10&ndash;20x</strong> from heap-allocated signal values vs memory-mapped registers, and <strong>2&ndash;5x</strong> from scheduler overhead.
  Block-level JIT would attack the first factor; fixing the signal representation would attack the second.
  The target is to be within <strong>1&ndash;2x of Xcelium</strong>, not 1,000x.
</p>

<h3>Formal Verification</h3>

<p>
  Simulation tests your design with specific inputs.
  Formal verification <em>proves</em> it correct for <em>all</em> inputs.
  <strong>Bounded model checking</strong> (BMC) exhaustively checks every possible state a design can reach within <em>k</em> clock cycles &mdash; if an assertion can be violated, BMC will find the exact input sequence that triggers it.
  <strong>Logic equivalence checking</strong> (LEC) proves that two implementations compute the same outputs for all inputs &mdash; essential after optimization, synthesis, or refactoring.
  Together, they catch bugs that no amount of random testing would find.
</p>

<p>
  The dominant open-source formal tool today is <a href="https://github.com/YosysHQ/sby">SymbiYosys</a> (sby), which wraps Yosys with a configuration-file-driven workflow.
  Here&rsquo;s the same verification task in both tools:
</p>

<pre><code><span style="color:#6b7280"># ── With SymbiYosys ──────────────────────────────────</span>
<span style="color:#6b7280"># 1. Create a .sby configuration file</span>
cat &gt; prim_count.sby &lt;&lt;EOF
[options]
mode bmc
depth 20

[engines]
smtbmc z3

[script]
read -formal prim_count.sv prim_count_tb.sv
prep -top prim_count_tb

[files]
prim_count.sv
prim_count_tb.sv
EOF

<span style="color:#6b7280"># 2. Run it</span>
sby prim_count.sby

<span style="color:#6b7280"># ── With CIRCT ────────────────────────────────────────</span>
<span style="color:#6b7280"># Same thing, two commands, no config file</span>
circt-verilog prim_count.sv prim_count_tb.sv -o prim_count.mlir
circt-bmc prim_count.mlir --bound 20 --module prim_count_tb</code></pre>

<p>
  Both use Z3 under the hood, but CIRCT&rsquo;s approach operates on MLIR &mdash; the same intermediate representation used for simulation.
  This means formal and simulation share the same frontend, the same lowering passes, and the same bug fixes.
  The fork also adds logic equivalence checking, which sby does not directly provide:
</p>

<pre><code><span style="color:#6b7280"># Check two implementations are logically equivalent</span>
circt-lec prim_count.mlir --first-module prim_count --second-module prim_count_opt</code></pre>

<p>Key capabilities:</p>
<ul>
  <li>Bounded model checking (BMC) with k-induction</li>
  <li>Logic equivalence checking (LEC)</li>
  <li>Z3 SMT solver integration</li>
  <li>Shared frontend with simulation (one parse, multiple backends)</li>
</ul>

<p>
  The formal tools are tested against three third-party assertion suites &mdash; the same ones used by other open-source EDA tools:
</p>

<table>
  <thead><tr><th>Suite</th><th>circt-bmc</th><th>circt-lec</th><th>sby bmc</th><th>sby lec</th></tr></thead>
  <tbody>
    <tr><td>verilator-verification</td><td>17/17 (100%)</td><td>17/17 (100%)</td><td>0/17 (0%)</td><td>&mdash;</td></tr>
    <tr><td>yosys SVA</td><td>14/14 (100%)</td><td>14/14 (100%)</td><td>14/14 (100%)</td><td>&mdash;</td></tr>
    <tr><td>sv-tests (formal subset)</td><td>23/26 (88%)</td><td>23/23 (100%)</td><td>3/26 (12%)</td><td>&mdash;</td></tr>
  </tbody>
</table>

<p>
  The sby column uses the open-source yosys (without the commercial Verific frontend).
  The yosys SVA suite was designed for yosys&rsquo;s own parser, so sby passes it at 100%.
  But verilator-verification and the sv-tests formal subset use concurrent SVA assertions (<code>assert property @(posedge clk) ...</code>, sequences, deferred immediates) that yosys&rsquo;s built-in parser cannot handle &mdash; it only supports simple immediate assertions like <code>initial assert(expr)</code>.
  circt-bmc handles all of these because it uses <a href="https://github.com/MikePopoloski/slang">slang</a> as its frontend, which has complete IEEE 1800 SystemVerilog support.
  The 3 sv-tests XFails in circt-bmc are due to SVA sequence operators not yet lowered to the Z3 backend.
</p>

<p>
  Beyond test suites, the formal tools work on real-world SoC designs.
  We verify modules from Google&rsquo;s <a href="https://opentitan.org/">OpenTitan</a> project (an open-source silicon root of trust) and lowRISC&rsquo;s <a href="https://github.com/lowRISC/ibex">Ibex</a> RISC-V core.
  For example, <code>circt-bmc</code> can prove that OpenTitan&rsquo;s <code>prim_count</code> (a counter primitive used throughout the SoC) has no assertion violations up to 20 cycles, and <code>circt-lec</code> can verify that Ibex&rsquo;s ALU matches a reference implementation.
  These aren&rsquo;t toy examples &mdash; OpenTitan is taped out silicon, and Ibex is deployed in production chips.
</p>

<h3>sv-tests: How We Compare</h3>

<p>
  The <a href="https://github.com/chipsalliance/sv-tests">CHIPS Alliance sv-tests</a> suite is the standard benchmark for SystemVerilog tool compliance, with ~1,600 tests covering the IEEE 1800-2017 specification chapter by chapter.
  Here&rsquo;s where circt-sim stands against other tools:
</p>

<figure class="wide">
  <img src="chart_sv_tests.png" alt="sv-tests pass rates: Slang 100%, circt 100%, Verilator 94.6%, Xcelium 81.1%, Icarus 72.1%">
  <figcaption>sv-tests pass rates. circt passes all 1,622 tests &mdash; ahead of Verilator (94.6%) and Cadence Xcelium (81.1%).</figcaption>
</figure>

<p>
  circt passes <strong>1,622 of 1,622</strong> sv-tests (100%), putting it ahead of Verilator (1,527/1,614, 94.6%), Cadence Xcelium (1,315/1,622, 81.1%), and Icarus Verilog (1,165/1,614, 72.1%).
  Yes, the open-source tool built in 43 days scores higher on IEEE 1800 compliance than the commercial simulator that costs six figures per seat.
  This includes all generated UVM class tests, global clocking sampled function tests, and complex core designs like black-parrot.
  The last 42 tests were fixed by a single slang preprocessor patch that correctly handles <code>`ifdef</code> directives inside <code>`define</code> macro bodies during skipped conditional branches.
</p>

<p>
  Getting to 100% was a gradual process.
  The chart below shows sv-tests pass rate measured at every 500 fork commits:
</p>

<figure class="wide">
  <img src="chart_sv_tests_progress.png" alt="sv-tests progress from 0% at fork start to 100% at commit 2968">
  <figcaption>sv-tests pass rate over time. Upstream CIRCT starts at 75% (1,211/1,622). The fork climbs steadily through ImportVerilog and MooreToCore work, reaching 90% by commit 800 and 99% by commit 1,500. The final 17 tests were fixed by a slang preprocessor patch at commit 2,968.</figcaption>
</figure>

<p>
  Most of these tests run in elaboration mode (1,505 of 1,622) &mdash; the same metric the sv-tests dashboard uses for all tools.
  The key difference is in what the tools can do <em>beyond</em> sv-tests: Verilator has no UVM support, no constrained random, no classes, and limited <code>fork</code>/<code>join</code>.
  circt-sim can run full UVM testbenches end-to-end &mdash; a capability that doesn&rsquo;t exist in any other open-source tool.
</p>

<h3>Mutation Testing</h3>

<p>
  The fork integrates with <a href="https://github.com/YosysHQ/mcy">MCY</a> (Mutation Coverage with Yosys) to measure test quality rather than just code coverage.
  The workflow is: mutate a design (flip an operator, change a constant, remove an assignment), then use formal verification to check whether your tests can detect the change.
</p>

<pre><code><span style="color:#6b7280"># Lower original and mutated designs to MLIR</span>
circt-verilog counter.sv counter_tb.sv -o original.mlir
circt-verilog counter_mutated.sv counter_tb.sv -o mutated.mlir

<span style="color:#6b7280"># Check if a mutation is detectable: prove the two designs differ</span>
circt-lec original.mlir --first-module counter --second-module counter_mutated

<span style="color:#6b7280"># Or check if the mutation breaks an assertion</span>
circt-bmc mutated.mlir --bound 20 --module counter_tb</code></pre>

<p>
  When a mutation is <em>killed</em> (the formal tool proves a difference or finds a violated assertion), it means your tests are sensitive to that fault.
  When a mutation <em>survives</em>, it means there&rsquo;s a gap in your test coverage.
  The fork&rsquo;s integration with MCY automates the generation, classification, and reporting of mutations across a design.
</p>

<hr>

<h2>What This Means for EDA</h2>

<p>
  The EDA industry has operated for decades on a simple assumption: verification tools are too complex for anything but large, well-funded teams to build.
  Xcelium and VCS represent millions of person-years of engineering effort.
</p>

<p>That assumption may no longer hold.</p>

<p>
  This fork is not a replacement for Xcelium &mdash; it's slower, less complete, and hasn't been hardened by decades of production use.
  But it demonstrates something important: <strong>a single engineer with AI assistance can build a functional verification stack in weeks, not years.</strong>
</p>

<p>
  The 580,000 lines of code in this fork are not all high-quality production code.
  Some of it is test infrastructure, some is scaffolding, and some will need significant rework.
  But the <em>capability surface</em> &mdash; simulation, VPI, UVM, formal, mutation &mdash; covers ground that would traditionally require a team of 20+ specialized engineers.
</p>

<p>I think this pattern will repeat across EDA:</p>
<ol>
  <li>Open-source infrastructure (CIRCT, MLIR) provides the foundation</li>
  <li>AI provides the volume &mdash; implementing specs, writing tests, building harnesses</li>
  <li>Human engineers provide the judgment &mdash; architecture, trade-offs, quality</li>
</ol>

<p>
  The cost of building verification tools is dropping by an order of magnitude.
  Whether that leads to better open-source tools, more specialized commercial tools, or something else entirely &mdash; I don't know.
  But the gap between &ldquo;possible&rdquo; and &ldquo;practical&rdquo; just got a lot smaller.
</p>

<p class="footer-note">
  The fork is at <a href="https://github.com/thomasnormal/circt">github.com/thomasnormal/circt</a>.
  Charts and data used in this post are in the <code>blog_data/</code> directory.
</p>

<hr>

<h2>Appendix: Week-by-Week Timeline</h2>

<p>
  The fork diverged from upstream CIRCT on <strong>January 9, 2026</strong> with a commit adding four-state logic support to the Arc dialect.
  The last commit at time of writing is from <strong>February 20</strong> &mdash; 43 days later.
  Here&rsquo;s how the features landed, week by week:
</p>

<div class="timeline-block">
  <p>Week 1 (Jan 9&ndash;12): Foundation</p>
  <ul>
    <li><strong><code>circt-sim</code> event-driven simulation driver</strong> &mdash; the core tool that takes MLIR generated by <code>circt-verilog</code> and executes it with IEEE 1800 scheduling semantics (active, inactive, NBA, observed, reactive regions).</li>
    <li><strong>Four-state (0/1/X/Z) type system</strong> &mdash; Verilog has four logic values, not two. Every signal, every operation, every comparison had to handle unknown (X) and high-impedance (Z) states correctly, or UVM testbenches produce garbage.</li>
    <li><strong>Coverage dialect</strong> &mdash; a new MLIR dialect for instrumenting simulation with functional coverage points, enabling coverage-driven verification workflows.</li>
    <li><strong>DPI-C runtime infrastructure</strong> &mdash; the foreign function interface that lets SystemVerilog call C code and vice versa, required for any real-world testbench that uses imported C functions.</li>
    <li><strong>UVM randomization ops</strong> &mdash; MLIR operations for constrained random generation, the foundation of UVM&rsquo;s stimulus generation where you declare constraints and the solver produces valid random values.</li>
  </ul>
</div>

<div class="timeline-block">
  <p>Week 2 (Jan 13&ndash;19): UVM Parity Push</p>
  <ul>
    <li><strong>UVM mailbox/semaphore primitives</strong> &mdash; inter-process communication channels that UVM sequences use to pass transactions between driver, monitor, and scoreboard components.</li>
    <li><strong><code>randomize()</code> method support</strong> &mdash; the SystemVerilog built-in that triggers constraint solving on class objects, called thousands of times per testbench run to generate stimulus.</li>
    <li><strong>Constraint expressions</strong> &mdash; support for <code>constraint</code> blocks with <code>inside</code>, <code>dist</code>, implication, and iterative constraints that define the legal space of random values.</li>
    <li><strong>AVIP (protocol verification IP) baseline</strong> &mdash; all 9 industry-standard protocol testbenches (APB, AHB, AXI4, SPI, JTAG, I2S, I3C) compiling successfully through the frontend for the first time.</li>
    <li><strong>Z3/JIT linking for BMC/LEC</strong> &mdash; connecting the Z3 SMT solver to CIRCT&rsquo;s formal verification tools, enabling them to actually prove or disprove assertions rather than just lowering them.</li>
  </ul>
</div>

<div class="timeline-block">
  <p>Weeks 3&ndash;4 (Jan 20&ndash;Feb 2): SystemVerilog Completeness</p>
  <ul>
    <li><strong>Hierarchical net resolution</strong> &mdash; support for cross-module references like <code>top.dut.clk</code>, which UVM monitors use to observe signals deep inside the design hierarchy without explicit port connections.</li>
    <li><strong><code>force</code>/<code>release</code>/<code>deassign</code> procedural statements</strong> &mdash; simulation-time overrides that let testbenches inject faults or override driver outputs, essential for testing error recovery paths.</li>
    <li><strong>UVM sequence/sequencer infrastructure</strong> &mdash; the runtime machinery that lets UVM sequences generate transactions, arbitrate between multiple sequences, and deliver items to drivers in the correct order.</li>
    <li><strong>sv-tests integration</strong> &mdash; connecting to the <a href="https://github.com/chipsalliance/sv-tests">sv-tests</a> community test suite, starting at 22 passing tests and rapidly growing as frontend gaps were filled.</li>
    <li><strong>OpenTitan <code>gpio_reg_top</code> end-to-end</strong> &mdash; the first real-world SoC module (from Google&rsquo;s <a href="https://opentitan.org/">OpenTitan</a> project) to simulate successfully from Verilog source to completion.</li>
    <li><strong>Verilator verification suite, yosys SVA at 78%</strong> &mdash; running CIRCT&rsquo;s formal tools against third-party test suites to measure how well we handle real-world SystemVerilog assertions.</li>
  </ul>
</div>

<div class="timeline-block">
  <p>Week 5 (Feb 3&ndash;9): Formal Verification + Mutation</p>
  <ul>
    <li><strong>BMC with k-induction and JIT compilation</strong> &mdash; extending bounded model checking from simple unrolling to k-induction (which can prove unbounded properties) and compiling the check to native code for 10x speedup.</li>
    <li><strong>Unified formal regression harness</strong> &mdash; a single <code>run_formal_all.sh</code> script that runs BMC, LEC, sv-tests, verilator verification, yosys SVA, and OpenTitan checks with quality gates and baseline tracking.</li>
    <li><strong>Mutation testing framework (<code>circt-mut</code>)</strong> &mdash; a tool that systematically mutates designs (flipping operators, changing constants, removing assignments) to measure whether your tests can detect each fault.</li>
    <li><strong>MCY integration</strong> &mdash; connecting to the <a href="https://github.com/YosysHQ/mcy">Mutation Coverage with Yosys</a> framework for standardized mutation generation and classification workflows.</li>
    <li><strong>800+ formal verification tests</strong> &mdash; comprehensive regression suite covering SVA properties, clock domain assertions, memory correctness, and arithmetic overflow checks.</li>
  </ul>
</div>

<div class="timeline-block">
  <p>Weeks 6&ndash;7 (Feb 10&ndash;20): Cocotb, VPI, and Hardening</p>
  <ul>
    <li><strong>VPI C API implementation (IEEE 1364/1800)</strong> &mdash; the standard programming interface that external tools use to interact with a running simulation, implementing signal discovery, value access, time callbacks, and simulation control.</li>
    <li><strong>Cocotb integration with four-state signal support</strong> &mdash; enabling Python-based testbenches via <a href="https://www.cocotb.org/">cocotb</a>, the most popular open-source verification framework, with proper X/Z handling.</li>
    <li><strong>Arcilator behavioral lowering</strong> &mdash; an alternative simulation backend that compiles CIRCT IR to native code via LLVM, trading interpretation overhead for faster execution of compute-heavy designs.</li>
    <li><strong>Unified regression orchestrator (40 lanes)</strong> &mdash; a manifest-driven system that runs all test suites (AVIP, sv-tests, verilator, yosys, cocotb, CVDP) across smoke/nightly/full profiles with retry logic and parity checking.</li>
  </ul>
</div>

</article>

</body>
</html>
